<!DOCTYPE html>
<html lang="en">
 <head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <style type="text/css" id="26760718606"></style> 
  <title>Yuanzhi Liang</title> 
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
  <link rel="stylesheet" href="./dist/css/bootstrap.css" /> 
  <link rel="stylesheet" type="text/css" href="./dist/css/screen.css" /> 
  <script src="./dist/js/slideShow.js"></script>
  <style>
	#slideShowImages { /* The following CSS rules are optional. */
	  border: 3px rgb(0, 0, 0) solid;
	  background-color: lightgray;
	}	
  
	#slideShowImages img { /* The following CSS rules are optional. */
	  border: 0.8em rgb(1, 3, 107) solid;
	  padding: 3px;
	}	
	ul.desp li{font-size:22px}
  </style>
  <style>
        .icon {
            margin-right: 10px;
            font-size: 24px; /* Adjust the size as needed */
        }
        .custom-icon {
            width: 24px; /* Adjust the size as needed */
            height: 24px;
            margin-right: 10px;
        }
  </style>
 </head> 


 <body style="font-size: 18px;"> 
  
  <nav class="navbar navbar-inverse navbar-fixed-top"> 
   <div class="container">
	<div class="navbar-header">

	 <a class="navbar-brand"><b>Yuanzhi Liang</b></a>
	</div>
	<div id="navbar" class="collapse navbar-collapse">
	 <ul class="nav navbar-nav">
			<li ><a href="#about"><b>Biography</b></a></li>
			<li ><a href="#exp"><b>Experience</b></a></li>
			<li ><a href="#honors"><b>Honors</b></a></li>
			<li ><a href="#publication"><b>Publications</b></a></li>
			<li ><a href="#academic"><b>Activities</b></a></li>
	 </ul> 
	</div> 
	
   </div> 
  </nav> 
  </div> 

	<br/><br/><br/>

    <div class="container">


		<div class="row">
			<div class="col-sm-5">
				<img src="./img/liang.jpg" width="120">
			</div>
			
			
			<div class="col-sm-7"><a id="contact"></a>
				<p> </p><h4>Yuanzhi Liang</h4>
				<p></p>
				<p>
					
				</p>

				<p>
					Mail: liangyzh18 [at] outlook [dot] com <br>
				</p>

				 <p>
					<a href="https://scholar.google.com/citations?user=YUjR-z8AAAAJ" target="_blank" style="margin-right: 10px;"><i class="fas fa-user-graduate" aria-hidden="true"></i></a>
					<a href="https://github.com/akira-l" target="_blank" style="margin-right: 10px;"><i class="fab fa-github" aria-hidden="true"></i></a>
					<a href="https://space.bilibili.com/3546680230152851" target="_blank" style="margin-right: 10px;"><img src="img/bilibili_icon.png" alt="Bilibili" width="25"></a>
					<a href="https://www.zhihu.com/people/akira-94-24" target="_blank" style="margin-right: 10px;"><img src="img/zhihu_icon.png" alt="zhihu" width="20"></a>
					<a href="https://www.xiaohongshu.com/user/profile/6419503a000000001201339a" target="_blank" style="margin-right: 10px;"><img src="img/xiaohongshu_icon.png" alt="Xiaohongshu" width="15"></a>
					<!-- <a href="www.linkedin.com/in/yuanzhi-liang-2aa77b28a" target="_blank" style="margin-right: 10px;"><i style="font-size:24px" class="fa">&#xf08c;</i></a> -->
				 </p>
			 </p>
			</div>
		</div>

	  
	  <a id="about"></a> <hr>
      <div class="row">
		<div class="span12">
			<h3>About Me</h3>
			   <p>
				I am a research scientist specializing in generative AI at the Institute of Artificial Intelligence (TeleAI), China Telecom. I received my Ph.D. from the University of Technology Sydney in 2024, advised by Dr. <a href="http://ffmpbgrnn.github.io/">Linchao Zhu</a> and Prof. <a href="https://scholar.google.com/citations?user=RMSuNFwAAAAJ&hl=zh-CN">Yi Yang</a>.
			  </p>
			  
			  <p>
				I received a Master's degree from Xi'an Jiaotong University in 2020 and was a member of the SMILES LAB, advised by Prof. <a href="https://scholar.google.com/citations?user=skQCiQQAAAAJ&hl=zh-CN">Xueming Qian</a> and Prof. <a href="http://gr.xjtu.edu.cn/web/zhuli">Li Zhu</a>.
			  </p>
			  
			<p>	
				
				My academic and professional journey is fueled by a dual curiosity: developing machines that perceive real-world environments and interpret complex semantics. My research interests include multimodal large models, video generation, 3D synthesis, and human-like AI agents. I am also interested in theoretical innovations in generative models.


			</p>
			  <div style="border: 2px solid #007acc; padding: 15px; border-radius: 5px; background-color: #eef6fc;">
				<p style="margin: 0;">
				  I am always looking for highly motivated research interns and long-term collaborators. We currently have multiple positions available, focusing on, but not limited to, multimodal large models, video generation/editing, and 3D generation. If you are interested in exploring these areas or discussing potential research collaborations, please feel free to contact me via email. (Applicants for internships are encouraged to include your CV.)
				</p>
			  </div>
			  

		</div>
      </div>


	  <a id="exp"></a> <hr>
      <div class="row">
		<div class="span12">
			<h3>Work Experience</h3>
			<ul>
			<li> Jul 2021 - Dec 2021, Alibaba DAMO Academy </li>
				<ul> 
					<li>Research intern working on virtual human synthesis.</li> 
				</ul> 

			<p>
			</p>

			<li> Jul 2020 - Jul 2021, Baidu Research  </li>
				<ul> 
					<li>Research intern working on visual knowledge embedding, object recognition, and multi-modal representation.</li> 
				</ul> 

			<p>		
			</p>

			<li> Mar 2020 - Jun 2020, JD AI Research </li>
				<ul> 
					<li>Research intern working on product recognition.</li> 
				</ul> 

			<p>	
			</p>

			<li> Aug 2018 - Jun 2019, JD AI Research </li>
				<ul> 
					<li>Research intern working on visual-language representation learning.</li> 
				</ul> 
			<p>	
			</p>

			</ul>
		</div>
      </div>


      <a id="honors"></a> <hr>
      <div class="row">
		<div class="span12">
			<h3>Selected Honors</h3>
			<ul>
			<li> <b>First place</b> in AliProducts Challenge @ CVPR 2020 the RetailVision workshop.</li>
			<li> <b>First place</b> in iMat Product Competition @ CVPR 2019 FGVC6 workshop.</li>
			<li> <b>First place</b> in in Fieldguide Challenge: Moths & Butterflies @ CVPR 2019 FGVC6 workshop.</li>
			<li> <b>Second place</b> in iFood Competition @ CVPR 2019 FGVC6 workshop.</li>
			<li> <b>Second place</b> in iMet2020 Fine-grained Attributes Classification Competition @ CVPR 2020 FGVC7 workshop.</li>
			<li> <b>Kaggle Silver Medal</b> in Deepfake Detection Challenge 2020.</li> 
			<!-- 
			<li> <b>Meritorious Winner</b> in Interdisciplinary Contest in Modeling (ICM) 2016.</li>
			<li> <b>First</b> Prize Scholarship in XJTU 2017 - 2019.</li>
			<li> <b>First, Third, Second</b> Prize Scholarship in LZU 2013 - 2016.</li>
			-->
			</ul>
		</div>
      </div>

      <a id="publication"></a>
      <hr>
	  
      <div class="row">
		<div class="span12">
			<h3>Important Preprints</h3>
			    <li>VAST 1.0: A Unified Framework for Controllable and Consistent Video Generation.  <br>Chi Zhang, <b>Yuanzhi Liang</b>, Xi Qiu, Fangqiu Yi, Xuelong Li.<br> Arxiv, 2024 <br><br></li>
				<li>AntEval: Quantitatively Evaluating Informativeness and Expressiveness of Agent Social Interactions.  <br><b>Yuanzhi Liang</b>, Linchao Zhu, Yi Yang.<br> Arxiv, 2024  <br><br></li>
				<li>Tachikuma: Understading Complex Interactions with Multi-Character and Novel Objects by Large Language Models.  <br><b>Yuanzhi Liang</b>, Linchao Zhu, Yi Yang.<br> Arxiv, 2023  <br><br></li>

			<h3>Selected Publications</h3>
				<li>MAAL: Multimodality-Aware Autoencoder-based Affordance Learning for 3D Articulated Objects.  <br><b>Yuanzhi Liang</b>, Xiaohan Wang, Linchao Zhu, Yi Yang.<br>Accepted by ICCV 2023 <br><br></li>

				<li>A Simple Episodic Linear Probe Improves Visual Recognition in the Wild.  <br><b>Yuanzhi Liang</b>, Linchao Zhu, Xiaohan Wang, Yi Yang.<br>Accepted by CVPR 2022 (Score 1/2/2) <br><br></li>

				<li>SEEG: Semantic Energized Co-speech Gesture Generation.  <br><b>Yuanzhi Liang</b>, Qianyu Feng, Linchao Zhu, Li Hu, Pan Pan, Yi Yang.<br>Accepted by CVPR 2022 <br><br></li> 

				<li>VrR-VG: Refocusing Visually-Relevant Relationships.  <br><b>Yuanzhi Liang</b>, Yalong Bai, Wei Zhang, Xueming Qian, Li Zhu, Tao Mei.<br>Accepted by ICCV 2019 <br><br></li>

				<li>IcoCap: Improving Video Captioning by Compounding Images. <br><b>Yuanzhi Liang</b>, Linchao Zhu, Xiaohan Wang, Yi Yang.<br>Accepted by TMM 2023 <br><br></li> 

				<li>Penalizing the Hard Example But Not Too Much: A Strong Baseline for Fine-Grained Visual Classification.  <br><b>Yuanzhi Liang</b>, Linchao Zhu, Xiaohan Wang, Yi Yang.<br> Accepted by TNNLS 2022 <br><br></li>

				<li>Towards Better Railway Service: Passengers Counting in Railway Compartment.  <br><b>Yuanzhi Liang</b>, Zhu Li, Xueming Qian.<br> Accepted by TCSVT 2020 <br><br></li>
				
				<li>Freelong: Training-free long video generation with spectralblend temporal attention. <br>Yu Lu, <b>Yuanzhi Liang</b>, Linchao Zhu, Yi Yang.<br> Accepted by NeurIPS 2024 <br><br></li>

				<li>Removing Raindrops and Rain Streaks in One Go. <br>Ruijie Quan, Xin Yu, <b>Yuanzhi Liang</b>, Yi Yang.<br>Accepted by CVPR 2021 <br><br></li>

				<li>Product Recognition for Unmanned Vending Machines. <br>Chengxu Liu, Zongyang Da, <b>Yuanzhi Liang</b>, Yao Xue, Guoshuai Zhao, Xueming Qian.<br> Accepted by TNNLS 2022 <br><br></li>

				<li>Food and Ingredient Joint Learning for Fine-Grained Recognition.  <br>Chengxu Liu, <b>Yuanzhi Liang</b>, Yao Xue, Xueming Qian, Jianlong Fu.<br> Accepted by TCSVT 2020 <br><br></li>

				
		</div>
      </div>


	  <a id="academic"></a>
	  <hr>

	  <div class="row">
		<div class="span12">
			<h3>Journal Reviewer</h3>
			<li>Reviewer for TPAMI and TIP.</li>
			<h3>Conference Reviewer / Program Committee Member</h3>
			<li>Reviewer for ICCV, CVPR, ICLR, NeurIPS, ECCV, MM, AAAI, IJCAI, ICME, and CAAI.</li>
			<h3>Others</h3>
			<li>Member of <a href="https://github.com/esbatmop/MNBVC">MNBVC (Massive Never-ending BT Vast Chinese corpus)</a>.</li>
		</div>
      </div>

	  <!--

	  <a id="cv"></a> <hr>
      <div class="row">
		<div class="span12">
			<h4>CV</h4>
			<p>
				More information about me: <a href="./cv/Yuanzhi Liang CV 2020.6.10.pdf">cv</a>	
			</p>
		</div>
      </div>

	  -->
      
	  <hr>

	  <div class="row">
		<div class="span12">
				<div align="center"> 
					<a href="https://clustrmaps.com/site/1bu57"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=k6KOcwz_2X8CFIy5DWZoKXDP4n2rVtUNCh82VD6Tw68&cl=ffffff" /></a>
				</div>
		</div>
      </div>

	</div> <!-- /container -->

	

    <!-- Le javascript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->

</div></body></html>
