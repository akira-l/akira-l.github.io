<!DOCTYPE html>
<html lang="en">
 <head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <style type="text/css" id="26760718606"></style> 
  <title>Yuanzhi Liang</title> 
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
  <link rel="stylesheet" href="./dist/css/bootstrap.css" /> 
  <link rel="stylesheet" type="text/css" href="./dist/css/screen.css" /> 
  <script src="./dist/js/slideShow.js"></script>
  <style>
	#slideShowImages { /* The following CSS rules are optional. */
	  border: 3px rgb(0, 0, 0) solid;
	  background-color: lightgray;
	}	
  
	#slideShowImages img { /* The following CSS rules are optional. */
	  border: 0.8em rgb(1, 3, 107) solid;
	  padding: 3px;
	}	
	ul.desp li{font-size:22px}
  </style>
  <style>
        .icon {
            margin-right: 10px;
            font-size: 24px; /* Adjust the size as needed */
        }
        .custom-icon {
            width: 24px; /* Adjust the size as needed */
            height: 24px;
            margin-right: 10px;
        }
  </style>
 </head> 


 <body style="font-size: 18px;"> 
  
  <nav class="navbar navbar-inverse navbar-fixed-top"> 
   <div class="container">
	<div class="navbar-header">

	 <a class="navbar-brand"><b>Yuanzhi Liang</b></a>
	</div>
	<div id="navbar" class="collapse navbar-collapse">
	 <ul class="nav navbar-nav">
			<li ><a href="#about"><b>Biography</b></a></li>
			<li ><a href="#exp"><b>Experience</b></a></li>
			<li ><a href="#honors"><b>honors</b></a></li>
			<li ><a href="#publication"><b>publications</b></a></li>
			<li ><a href="#academic"><b>Activities</b></a></li>
	 </ul> 
	</div> 
	
   </div> 
  </nav> 
  </div> 

	<br/><br/><br/>

    <div class="container">


		<div class="row">
			<div class="col-sm-5">
				<img src="./img/liang.jpg" width="120">
			</div>
			
			
			<div class="col-sm-7"><a id="contact"></a>
				<p> </p><h4>Yuanzhi Liang</h4>
				<p></p>
				<p>
					
				</p>

				<p>
					Mail: liangyzh18 [at] outlook [dot] com <br>
				</p>

				 <p>
					<a href="https://scholar.google.com/citations?user=YUjR-z8AAAAJ" target="_blank" style="margin-right: 10px;"><i class="fas fa-user-graduate" aria-hidden="true"></i></a>
					<a href="https://github.com/akira-l" target="_blank" style="margin-right: 10px;"><i class="fab fa-github" aria-hidden="true"></i></a>
					<a href="https://space.bilibili.com/3546680230152851" target="_blank" style="margin-right: 10px;"><img src="img/bilibili_icon.png" alt="Bilibili" width="25"></a>
					<a href="https://www.zhihu.com/people/akira-94-24" target="_blank" style="margin-right: 10px;"><img src="img/zhihu_icon.png" alt="zhihu" width="20"></a>
					<a href="https://www.xiaohongshu.com/user/profile/6419503a000000001201339a" target="_blank" style="margin-right: 10px;"><img src="img/xiaohongshu_icon.png" alt="Xiaohongshu" width="20"></a>
				 </p>
			 </p>
			</div>
		</div>

	  
	  <a id="about"></a> <hr>
      <div class="row">
		<div class="span12">
			<h3>About Me</h3>
			<p>	
				I am currently a Ph.D. student at the University of Technology Sydney, advised by Dr. <a href="http://ffmpbgrnn.github.io/">Linchao Zhu</a> and Prof. <a href="https://scholar.google.com/citations?user=RMSuNFwAAAAJ&hl=zh-CN">Yi Yang</a>.
				I received a Master's degree from Xi'an Jiaotong Univerisity in 2020. I was a member of the SMILES LAB, advised by Prof. <a href="https://scholar.google.com/citations?user=skQCiQQAAAAJ&hl=zh-CN">Xueming Qian</a> and Prof. <a href="http://gr.xjtu.edu.cn/web/zhuli">Li Zhu</a>. 
				<br>

				<br>
				My academic and professional journey has been driven by two kinds of curiosities: the development of machines capable of perceiving real-world scenarios and understanding semantics.
				<br>

				<br>
				My primary research interests include:
				<br>
				1. Visual Perception in Real-world Scenarios: Tackling the challenges of visual perception in real-world scenarios and exploring their relevant applications.
				<br>
				2. Multi-modal Representation Learning: Investigating representation learning for multi-modal data, with a special focus on visual-language tasks. 
				<br>

				<br>
				Lately, my research trajectory has evolved:
				<br>
				1. Perception in Interactive Environments: I'm delving deeper into addressing perception challenges in interactive settings. A significant portion of this line is dedicated to improving the capabilities of robotic systems, enabling them to interpret and interact seamlessly within real-world environments.
				<br>
				2. Development of Humanoid AI Agents Using Large Models: I'm captivated by the possibilities large models offer in the creation of humanoid AI agents. The overarching objective is to mimic human-like intelligence and behavioral patterns more closely.
				<br>

				<!--My work has primarily revolved around: 1. visual perception in real-world scenarios; 2. representation learning for multi-modal data, especially for visual-language tasks. 
				Recently, my pursuits have expanded to 1. refining perception ability in interactable environments, especially for robots; 2. constructing humanoid AI agent by large models. 
				Lately, I've delved into 1. enhancing perception in interactive environments; 2. harnessing large models to explore humanoid AI agent capabilities.
				My previous research explore two lines. 1. Visual perception in general real-world scenarios and related real-world problems. 2. More than visual modality, perceive and reason multi-modal information from the real-world environments and bridge to the semantic in human language. Recently, I go further with above directions. My research go further with two folds. 1. Solving perception problem considering the interactive environment close to the real world. 2. Empower high humanoid AI agent by large models. -->
			</p>
		</div>
      </div>


	  <a id="exp"></a> <hr>
      <div class="row">
		<div class="span12">
			<h3>Work Experience</h3>
			<ul>
			<li> Jul 2021 - Dec 2021, Alibaba DAMO Academy </li>
				<ul> 
					<li>Research intern working on virtual human synthesis.</li> 
				</ul> 

			<p>
			</p>

			<li> Jul 2020 - Jul 2021, Baidu Research  </li>
				<ul> 
					<li>Research intern working on visual knowledge embedding, object recognition, and multi-modal representation.</li> 
				</ul> 

			<p>		
			</p>

			<li> Mar 2020 - Jun 2020, JD AI Research </li>
				<ul> 
					<li>Research intern working on product recognition.</li> 
				</ul> 

			<p>	
			</p>

			<li> Aug 2018 - Jun 2019, JD AI Research </li>
				<ul> 
					<li>Research intern working on visual-language representation learning.</li> 
				</ul> 
			<p>	
			</p>

			</ul>
		</div>
      </div>


      <a id="honors"></a> <hr>
      <div class="row">
		<div class="span12">
			<h3>Selected Honors</h3>
			<ul>
			<li> <b>First place</b> in AliProducts Challenge @ CVPR 2020 the RetailVision workshop.</li>
			<li> <b>First place</b> in iMat Product Competition @ CVPR 2019 FGVC6 workshop.</li>
			<li> <b>First place</b> in in Fieldguide Challenge: Moths & Butterflies @ CVPR 2019 FGVC6 workshop.</li>
			<li> <b>Second place</b> in iFood Competition @ CVPR 2019 FGVC6 workshop.</li>
			<li> <b>Second place</b> in iMet2020 Fine-grained Attributes Classification Competition @ CVPR 2020 FGVC7 workshop.</li>
			<li> <b>Kaggle Silver Medal</b> in Deepfake Detection Challenge 2020.</li> 
			<!-- 
			<li> <b>Meritorious Winner</b> in Interdisciplinary Contest in Modeling (ICM) 2016.</li>
			<li> <b>First</b> Prize Scholarship in XJTU 2017 - 2019.</li>
			<li> <b>First, Third, Second</b> Prize Scholarship in LZU 2013 - 2016.</li>
			-->
			</ul>
		</div>
      </div>

      <a id="publication"></a>
      <hr>
	  
      <div class="row">
		<div class="span12">
			<h3>Selected Publications</h3>
				<li>MAAL: Multimodality-Aware Autoencoder-based Affordance Learning for 3D Articulated Objects.  <br><b>Yuanzhi Liang</b>, Xiaohan Wang, Linchao Zhu, Yi Yang.<br>Accepted by ICCV 2023 <br><br></li>

				<li>A Simple Episodic Linear Probe Improves Visual Recognition in the Wild.  <br><b>Yuanzhi Liang</b>, Linchao Zhu, Xiaohan Wang, Yi Yang.<br>Accepted by CVPR 2022 (Score 1/2/2) <br><br></li>

				<li>SEEG: Semantic Energized Co-speech Gesture Generation.  <br><b>Yuanzhi Liang</b>, Qianyu Feng, Linchao Zhu, Li Hu, Pan Pan, Yi Yang.<br>Accepted by CVPR 2022 <br><br></li>

				<li>VrR-VG: Refocusing Visually-Relevant Relationships.  <br><b>Yuanzhi Liang</b>, Yalong Bai, Wei Zhang, Xueming Qian, Li Zhu, Tao Mei.<br>Accepted by ICCV 2019 <br><br></li>

				<li>IcoCap: Improving Video Captioning by Compounding Images. <br><b>Yuanzhi Liang</b>, Linchao Zhu, Xiaohan Wang, Yi Yang.<br>Accepted by TMM 2023 <br><br></li> 

				<li>Penalizing the Hard Example But Not Too Much: A Strong Baseline for Fine-Grained Visual Classification.  <br><b>Yuanzhi Liang</b>, Linchao Zhu, Xiaohan Wang, Yi Yang.<br> Accepted by TNNLS 2022 <br><br></li>

				<li>Towards Better Railway Service: Passengers Counting in Railway Compartment.  <br><b>Yuanzhi Liang</b>, Zhu Li, Xueming Qian.<br> Accepted by TCSVT 2020 <br><br></li>

				<li>Tachikuma: Understading Complex Interactions with Multi-Character and Novel Objects by Large Language Models.  <br><b>Yuanzhi Liang</b>, Linchao Zhu, Yi Yang.<br> Ongoing work on arXiv <br><br></li>
				
				<li>Removing Raindrops and Rain Streaks in One Go. <br>Ruijie Quan, Xin Yu, <b>Yuanzhi Liang</b>, Yi Yang.<br>Accepted by CVPR 2021 <br><br></li>

				<li>Product Recognition for Unmanned Vending Machines. <br>Chengxu Liu, Zongyang Da, <b>Yuanzhi Liang</b>, Yao Xue, Guoshuai Zhao, Xueming Qian.<br> Accepted by TNNLS 2022 <br><br></li>

				<li>Food and Ingredient Joint Learning for Fine-Grained Recognition.  <br>Chengxu Liu, <b>Yuanzhi Liang</b>, Yao Xue, Xueming Qian, Jianlong Fu.<br> Accepted by TCSVT 2020 <br><br></li>
				
		</div>
      </div>


	  <a id="academic"></a>
	  <hr>

	  <div class="row">
		<div class="span12">
			<h3>Academic Services</h3>
			<li>Reviewer for TPAMI, TIP, ICCV, CVPR, ECCV, MM, ICME, KBS and CAAI. </li>
		</div>
      </div>

	  <!--

	  <a id="cv"></a> <hr>
      <div class="row">
		<div class="span12">
			<h4>CV</h4>
			<p>
				More information about me: <a href="./cv/Yuanzhi Liang CV 2020.6.10.pdf">cv</a>	
			</p>
		</div>
      </div>

	  -->
      
	  <hr>

	  <div class="row">
		<div class="span12">
				<div align="center"> 
					<a href="https://clustrmaps.com/site/1bu57"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=k6KOcwz_2X8CFIy5DWZoKXDP4n2rVtUNCh82VD6Tw68&cl=ffffff" /></a>
				</div>
		</div>
      </div>

	</div> <!-- /container -->

	

    <!-- Le javascript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->

</div></body></html>
