<!DOCTYPE html>
<html lang="en">
 <head>
  <meta charset="UTF-8">
   <meta name="viewport" content="width=device-width, initial-scale=1.0">
   <meta name="description" content="Yuanzhi Liang — Research Scientist at TeleAI, China Telecom. PhD from UTS. Specializing in generative AI, post-training, alignment, video generation, and world models.">
   <meta name="keywords" content="Yuanzhi Liang, 梁远智, generative AI, post-training, alignment, video generation, world models, TeleAI, China Telecom, computer vision, deep learning">
   <meta name="author" content="Yuanzhi Liang">
   <link rel="canonical" href="https://akira-l.github.io/">

   <!-- Open Graph -->
   <meta property="og:title" content="Yuanzhi Liang — Research Scientist | Generative AI, Post-training & Alignment">
   <meta property="og:description" content="Personal homepage of Yuanzhi Liang, Research Scientist at TeleAI (China Telecom). Research on generative AI, post-training, alignment, video generation, and world models. PhD from University of Technology Sydney.">
   <meta property="og:type" content="website">
   <meta property="og:url" content="https://akira-l.github.io/">
   <meta property="og:image" content="https://akira-l.github.io/img/kuku.jpg">
   <meta property="og:image:alt" content="Kuku the cat — post-training advisor of Yuanzhi Liang">
   <meta property="og:site_name" content="Yuanzhi Liang">
   <meta property="og:locale" content="en_US">

   <!-- Twitter Card -->
   <meta name="twitter:card" content="summary">
   <meta name="twitter:title" content="Yuanzhi Liang — Research Scientist | Generative AI">
   <meta name="twitter:description" content="Research Scientist at TeleAI. Specializing in generative AI, post-training, alignment, video generation, and world models.">
   <meta name="twitter:image" content="https://akira-l.github.io/img/kuku.jpg">
   <meta name="twitter:image:alt" content="Kuku the cat — post-training advisor of Yuanzhi Liang">

   <!-- JSON-LD Structured Data -->
   <script type="application/ld+json">
   {
     "@context": "https://schema.org",
     "@graph": [
       {
         "@type": "WebSite",
         "@id": "https://akira-l.github.io/#website",
         "url": "https://akira-l.github.io/",
         "name": "Yuanzhi Liang",
         "description": "Personal homepage of Yuanzhi Liang, Research Scientist specializing in generative AI."
       },
       {
         "@type": "WebPage",
         "@id": "https://akira-l.github.io/#webpage",
         "url": "https://akira-l.github.io/",
         "name": "Yuanzhi Liang — Research Scientist | Generative AI, Post-training & Alignment",
         "isPartOf": { "@id": "https://akira-l.github.io/#website" },
         "about": { "@id": "https://akira-l.github.io/#person" },
          "dateModified": "2026-02-13"
       },
       {
         "@type": "Person",
         "@id": "https://akira-l.github.io/#person",
         "name": "Yuanzhi Liang",
         "alternateName": ["梁远智", "Yuanzhi Liang", "Yuanzhi (Liam) Liang", "akira-l"],
         "url": "https://akira-l.github.io/",
         "image": "https://akira-l.github.io/img/kuku.jpg",
         "jobTitle": "Research Scientist",
         "worksFor": {
           "@type": "Organization",
           "name": "TeleAI, China Telecom",
           "url": "https://www.chinatelecom.com.cn/"
         },
         "alumniOf": [
           {
             "@type": "CollegeOrUniversity",
             "name": "University of Technology Sydney",
             "url": "https://www.uts.edu.au/"
           },
           {
             "@type": "CollegeOrUniversity",
             "name": "Xi'an Jiaotong University",
             "url": "http://www.xjtu.edu.cn/"
           }
         ],
         "knowsAbout": [
           "Generative AI",
           "Post-training",
           "Alignment",
           "Video Generation",
           "World Models",
           "Computer Vision",
           "Multimodal Learning",
           "3D Generation"
         ],
         "sameAs": [
           "https://scholar.google.com/citations?user=YUjR-z8AAAAJ",
           "https://github.com/akira-l",
           "https://www.linkedin.com/in/yuanzhi-liang-2aa77b28a/",
           "https://space.bilibili.com/3546680230152851",
           "https://www.zhihu.com/people/cyqwklp",
           "https://www.xiaohongshu.com/user/profile/6419503a000000001201339a"
         ]
       }
     ]
   }
   </script>

   <style type="text/css" id="26760718606"></style> 
   <title>Yuanzhi Liang — Research Scientist | Generative AI, Post-training & Alignment</title> 
   <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <link rel="stylesheet" href="./dist/css/bootstrap.css" /> 
   <link rel="stylesheet" type="text/css" href="./dist/css/screen.css" /> 
  <script src="./dist/js/slideShow.js"></script>
  <style>
	#slideShowImages { /* The following CSS rules are optional. */
	  border: 3px rgb(0, 0, 0) solid;
	  background-color: lightgray;
	}	
  
	#slideShowImages img { /* The following CSS rules are optional. */
	  border: 0.8em rgb(1, 3, 107) solid;
	  padding: 3px;
	}	
	ul.desp li{font-size:22px}
  </style>

  <style>
    .pub-badges a {
      display: inline-block;
      margin-left: 6px;
      padding: 2px 8px;
      border-radius: 999px;
      border: 1px solid rgba(0,0,0,0.18);
      font-size: 12px;
      line-height: 1.6;
      text-decoration: none;
      vertical-align: baseline;
    }

    .pub-badges a.badge-tldr {
      color: #0f766e;
      background: rgba(15, 118, 110, 0.08);
      border-color: rgba(15, 118, 110, 0.28);
    }

    .pub-badges a.badge-home {
      color: #2c4a67;
      background: rgba(44, 74, 103, 0.08);
      border-color: rgba(44, 74, 103, 0.22);
    }

    .pub-badges a.badge-paper {
      color: #6b4e00;
      background: rgba(214, 158, 46, 0.14);
      border-color: rgba(214, 158, 46, 0.32);
    }

    .pub-badges a:hover {
      filter: brightness(0.96);
      text-decoration: none;
    }
  </style>
  <style>
        .icon {
            margin-right: 10px;
            font-size: 24px; /* Adjust the size as needed */
        }
        .custom-icon {
            width: 24px; /* Adjust the size as needed */
            height: 24px;
            margin-right: 10px;
        }
  </style>
 </head> 


 <body style="font-size: 18px;"> 
  
  <nav class="navbar navbar-inverse navbar-fixed-top"> 
   <div class="container">
	<div class="navbar-header">

	 <a class="navbar-brand"><b>Yuanzhi Liang</b></a>
	</div>
	<div id="navbar" class="collapse navbar-collapse">
	 <ul class="nav navbar-nav">
			<li ><a href="#about"><b>Biography</b></a></li>
			<li ><a href="#exp"><b>Experience</b></a></li>
			<li ><a href="#honors"><b>Honors</b></a></li>
			<li ><a href="#publication"><b>Publications</b></a></li>
			<li ><a href="#academic"><b>Activities</b></a></li>
	 </ul> 
	</div> 
	
   </div> 
   </nav> 

	<br/><br/><br/>

    <div class="container">


		<div class="row">
			<div class="col-sm-5" style="text-align:center;">
				<img src="./img/kuku.jpg" width="120"><br>
				<small><em>kuku — my post-training advisor; specializes in comfort priors, household physics, and alignment with human preferences.</em></small>
			</div>
			
			
			<div class="col-sm-7"><a id="contact"></a>
				<p> </p><h1 style="font-size: 24px; margin-top: 0;">Yuanzhi Liang</h1>
				<p></p>
				<p>
					
				</p>

				<p>
					Mail: liangyzh18 [at] outlook [dot] com <br>
				</p>

				 <p>
					<a href="https://scholar.google.com/citations?user=YUjR-z8AAAAJ" target="_blank" rel="noopener" style="margin-right: 10px;" aria-label="Google Scholar profile of Yuanzhi Liang"><i class="fas fa-user-graduate" aria-hidden="true"></i></a>
					<a href="https://github.com/akira-l" target="_blank" rel="noopener" style="margin-right: 10px;" aria-label="GitHub profile of Yuanzhi Liang"><i class="fab fa-github" aria-hidden="true"></i></a>
					<a href="https://www.linkedin.com/in/yuanzhi-liang-2aa77b28a/" target="_blank" rel="noopener" style="margin-right: 10px;" aria-label="LinkedIn profile of Yuanzhi Liang"><i class="fab fa-linkedin" aria-hidden="true"></i></a>
					<a href="https://space.bilibili.com/3546680230152851" target="_blank" rel="noopener" style="margin-right: 10px;" aria-label="Bilibili channel of Yuanzhi Liang"><img src="img/bilibili_icon.png" alt="Bilibili icon" width="25"></a>
					<a href="https://www.zhihu.com/people/cyqwklp" target="_blank" rel="noopener" style="margin-right: 10px;" aria-label="Zhihu profile of Yuanzhi Liang"><img src="img/zhihu_icon.png" alt="Zhihu icon" width="20"></a>
					<a href="https://www.xiaohongshu.com/user/profile/6419503a000000001201339a" target="_blank" rel="noopener" style="margin-right: 10px;" aria-label="Xiaohongshu profile of Yuanzhi Liang"><img src="img/xiaohongshu_icon.png" alt="Xiaohongshu icon" width="15"></a>
				 </p>
			 </p>
			</div>
		</div>

	  
	  <a id="about"></a> <hr>
      <div class="row">
		<div class="span12">
			<h3>About Me</h3>
			   <p>
				I am a research scientist specializing in generative AI at the Institute of Artificial Intelligence (TeleAI), China Telecom. I received my Ph.D. from the University of Technology Sydney in 2024, advised by Dr. <a href="http://ffmpbgrnn.github.io/">Linchao Zhu</a> and Prof. <a href="https://scholar.google.com/citations?user=RMSuNFwAAAAJ&hl=zh-CN">Yi Yang</a>.
			  </p>
			  
			  <p>
				I received a Master's degree from Xi'an Jiaotong University in 2020 and was a member of the SMILES LAB, advised by Prof. <a href="https://scholar.google.com/citations?user=skQCiQQAAAAJ&hl=zh-CN">Xueming Qian</a> and Prof. <a href="http://gr.xjtu.edu.cn/web/zhuli">Li Zhu</a>.
			  </p>
			  
			<p>	
				<!-- My research focuses on improving machine intelligence that learns and generalizes from visual content, with particular interests in post-training of generative visual systems, video generation, and world models. I am also interested in theoretical innovations in generative models. -->

				My research focuses on generative AI with an emphasis on post-training, alignment, and structured world understanding built on large-scale data-driven foundations. I am interested in how models develop coherent internal representations, respect real-world structure, and refine their behavior through principled signals—including physical priors, aesthetic objectives, and human-aligned guidance. The aim is to move toward generative systems that remain grounded in data while becoming more reliable, structurally consistent, and capable of progressive self-improvement.
				</p>

				<p>
				More broadly, I am interested in learning frameworks where generative models improve through simulation, interaction, and self-directed refinement rather than static supervision alone. This includes the use of world models, controlled training environments, and structured feedback to provide richer learning signals—enabling models to test hypotheses, adapt, and grow through experience. The long-term goal is to support a trajectory toward generative intelligence that is grounded, aligned, and continuously advancing through both data and experience.


			</p>
			  <div style="border: 2px solid #007acc; padding: 15px; border-radius: 5px; background-color: #eef6fc;">
				<p style="margin: 0;">
				  I am always looking for highly motivated research interns and long-term collaborators. We currently have multiple positions available, focusing on, but not limited to, multimodal large models, video generation/editing, and 3D generation. If you are interested in exploring these areas or discussing potential research collaborations, please feel free to contact me via email. (Applicants for internships are encouraged to include your CV.)
				</p>
			  </div>
			  

		</div>
      </div>


	  <a id="exp"></a> <hr>
      <div class="row">
		<div class="span12">
			<h3>Work Experience</h3>
			<ul>
			<li> Jul 2021 - Dec 2021, Alibaba DAMO Academy </li>
				<ul> 
					<li>Research intern working on virtual human synthesis.</li> 
				</ul> 

			<p>
			</p>

			<li> Jul 2020 - Jul 2021, Baidu Research  </li>
				<ul> 
					<li>Research intern working on visual knowledge embedding, object recognition, and multi-modal representation.</li> 
				</ul> 

			<p>		
			</p>

			<li> Mar 2020 - Jun 2020, JD AI Research </li>
				<ul> 
					<li>Research intern working on product recognition.</li> 
				</ul> 

			<p>	
			</p>

			<li> Aug 2018 - Jun 2019, JD AI Research </li>
				<ul> 
					<li>Research intern working on visual-language representation learning.</li> 
				</ul> 
			<p>	
			</p>

			</ul>
		</div>
      </div>


      <a id="honors"></a> <hr>
      <div class="row">
		<div class="span12">
			<h3>Selected Honors</h3>
			<ul>
			<li> <b>First place</b> in AliProducts Challenge @ CVPR 2020 the RetailVision workshop.</li>
			<li> <b>First place</b> in iMat Product Competition @ CVPR 2019 FGVC6 workshop.</li>
			<li> <b>First place</b> in in Fieldguide Challenge: Moths & Butterflies @ CVPR 2019 FGVC6 workshop.</li>
			<li> <b>Second place</b> in iFood Competition @ CVPR 2019 FGVC6 workshop.</li>
			<li> <b>Second place</b> in iMet2020 Fine-grained Attributes Classification Competition @ CVPR 2020 FGVC7 workshop.</li>
			<li> <b>Kaggle Silver Medal</b> in Deepfake Detection Challenge 2020.</li> 
			<!-- 
			<li> <b>Meritorious Winner</b> in Interdisciplinary Contest in Modeling (ICM) 2016.</li>
			<li> <b>First</b> Prize Scholarship in XJTU 2017 - 2019.</li>
			<li> <b>First, Third, Second</b> Prize Scholarship in LZU 2013 - 2016.</li>
			-->
			</ul>
		</div>
      </div>

      <a id="publication"></a>
      <hr>
	  
      <div class="row">
		<div class="span12">
		<h3>Important Preprints <small style="font-size: 14px; color: #666;">(<a href="./publications/index.html">all publications</a>)</small></h3>
			    <li>TeleBoost: A Systematic Alignment Framework for High-Fidelity, Controllable, and Robust Video Generation <span class="pub-badges"><a class="badge-tldr" href="./publications/teleboost/index.html">TL;DR</a><a class="badge-home" href="https://tele-ai.github.io/TeleBoost/" target="_blank" rel="noopener">Homepage</a><a class="badge-paper" href="https://arxiv.org/abs/2602.07595" target="_blank" rel="noopener">Link</a></span><br><b>Yuanzhi Liang</b>, Xuan'er Wu, Yirui Liu, Yijie Fang, Yizhen Fan, Ke Hao, Rui Li, Ruiying Liu, Ziqi Ni, Peng Yu, Yanbo Wang, Haibin Huang, Qizhen Weng, Chi Zhang, Xuelong Li.<br> arXiv, 2026 <br><br></li>
			    <li>TeleWorld: Towards Dynamic Multimodal Synthesis with a 4D World Model <br>Yabo Chen, <b>Yuanzhi Liang</b>, Jiepeng Wang, Tingxi Chen, Junfei Cheng, Zixiao Gu, Yuyang Huang, Zicheng Jiang, Wei Li, Tian Li, Weichen Li, Zuoxin Li, Guangce Liu, Jialun Liu, Junqi Liu, Haoyuan Wang, Qizhen Weng, Xuan'er Wu, Xunzhi Xiang, Xiaoyan Yang, Xin Zhang, Shiwen Zhang, Junyu Zhou, Chengcheng Zhou, Haibin Huang, Chi Zhang, Xuelong Li.<br> arXiv, 2025 <br><br></li>
			    <li>Seeing What Matters: Visual Preference Policy Optimization for Visual Generation <br>Ziqi Ni*, <b>Yuanzhi Liang</b>, Rui Li, Yi Zhou, Haibing Huang, Chi Zhang, Xuelong Li.<br> arXiv, 2025 <br><br></li>
			    <li>Growing with the Generator: Self-paced GRPO for Video Generation <br>Rui Li*, <b>Yuanzhi Liang</b>, Ziqi Ni, Haibing Huang, Chi Zhang, Xuelong Li.<br> arXiv, 2025 <br><br></li>
			    <li>Learning What to Trust: Bayesian Prior-Guided Optimization for Visual Generation <br>Ruiying Liu*, <b>Yuanzhi Liang</b>, Haibin Huang, Tianshu Yu, Chi Zhang.<br> arXiv, 2025 <br><br></li>
			    <!-- <li>CtrlVDiff: Controllable Video Generation via Unified Multimodal Video Diffusion <br>Dianbing Xi, Jiepeng Wang, <b>Yuanzhi Liang</b>, Xi Qiu, Jialun Liu, Hao Pan, Yuchi Huo, Rui Wang, Haibin Huang, Chi Zhang, Xuelong Li.<br> Arxiv, 2025 <br><br></li> -->
			    <li>Free3D: 3D Human Motion Emerges from Single-View 2D Supervision <br>Sheng Liu*, <b>Yuanzhi Liang</b>, Sidan Du.<br> arXiv, 2025 <br><br></li>
			    <li>VAST 1.0: A Unified Framework for Controllable and Consistent Video Generation.  <br>Chi Zhang, <b>Yuanzhi Liang</b>, Xi Qiu, Fangqiu Yi, Xuelong Li.<br> arXiv, 2024 <br><br></li>
				<!-- <li>AntEval: Quantitatively Evaluating Informativeness and Expressiveness of Agent Social Interactions.  <br><b>Yuanzhi Liang</b>, Linchao Zhu, Yi Yang.<br> Arxiv, 2024  <br><br></li> -->
				<!-- <li>Tachikuma: Understading Complex Interactions with Multi-Character and Novel Objects by Large Language Models.  <br><b>Yuanzhi Liang</b>, Linchao Zhu, Yi Yang.<br> Arxiv, 2023  <br><br></li> -->

			<h3>Selected Publications</h3>
				<li>Uni-Inter: Unifying 3D Human Motion Synthesis Across Diverse Interaction Contexts <span class="pub-badges"><a class="badge-tldr" href="./publications/uni-inter/index.html">TL;DR</a><a class="badge-paper" href="https://dl.acm.org/doi/full/10.1145/3757377.3763954" target="_blank" rel="noopener">Link</a></span><br>Sheng Liu*, <b>Yuanzhi Liang</b>, Jiepeng Wang, Sidan Du, Chi Zhang, Xuelong Li.<br> Accepted by SIGGRAPH Asia 2025 <br><br></li>

				<li>MAAL: Multimodality-Aware Autoencoder-based Affordance Learning for 3D Articulated Objects. <span class="pub-badges"><a class="badge-tldr" href="./publications/maal/index.html">TL;DR</a><a class="badge-paper" href="https://openaccess.thecvf.com/content/ICCV2023/html/Liang_MAAL_Multimodality-Aware_Autoencoder-Based_Affordance_Learning_for_3D_Articulated_Objects_ICCV_2023_paper.html" target="_blank" rel="noopener">Link</a></span> <br><b>Yuanzhi Liang</b>, Xiaohan Wang, Linchao Zhu, Yi Yang.<br>Accepted by ICCV 2023 <br><br></li>

				<li>A Simple Episodic Linear Probe Improves Visual Recognition in the Wild. <span class="pub-badges"><a class="badge-tldr" href="./publications/elp/index.html">TL;DR</a><a class="badge-paper" href="https://openaccess.thecvf.com/content/CVPR2022/html/Liang_A_Simple_Episodic_Linear_Probe_Improves_Visual_Recognition_in_the_CVPR_2022_paper.html" target="_blank" rel="noopener">Link</a></span> <br><b>Yuanzhi Liang</b>, Linchao Zhu, Xiaohan Wang, Yi Yang.<br>Accepted by CVPR 2022 (Score 1/2/2) <br><br></li>

				<li>SEEG: Semantic Energized Co-speech Gesture Generation. <span class="pub-badges"><a class="badge-tldr" href="./publications/seeg/index.html">TL;DR</a><a class="badge-paper" href="https://openaccess.thecvf.com/content/CVPR2022/html/Liang_SEEG_Semantic_Energized_Co-Speech_Gesture_Generation_CVPR_2022_paper.html" target="_blank" rel="noopener">Link</a></span> <br><b>Yuanzhi Liang</b>, Qianyu Feng, Linchao Zhu, Li Hu, Pan Pan, Yi Yang.<br>Accepted by CVPR 2022 <br><br></li> 

				<li>VrR-VG: Refocusing Visually-Relevant Relationships. <span class="pub-badges"><a class="badge-tldr" href="./publications/vrr-vg/index.html">TL;DR</a><a class="badge-paper" href="https://openaccess.thecvf.com/content_ICCV_2019/html/Liang_VrR-VG_Refocusing_Visually-Relevant_Relationships_ICCV_2019_paper.html" target="_blank" rel="noopener">Link</a></span> <br><b>Yuanzhi Liang</b>, Yalong Bai, Wei Zhang, Xueming Qian, Li Zhu, Tao Mei.<br>Accepted by ICCV 2019 <br><br></li>

				<li>IcoCap: Improving Video Captioning by Compounding Images. <span class="pub-badges"><a class="badge-tldr" href="./publications/icocap/index.html">TL;DR</a><a class="badge-paper" href="https://ieeexplore.ieee.org/abstract/document/10272675" target="_blank" rel="noopener">Link</a></span><br><b>Yuanzhi Liang</b>, Linchao Zhu, Xiaohan Wang, Yi Yang.<br>Accepted by TMM 2023 <br><br></li> 

				<li>Penalizing the Hard Example But Not Too Much: A Strong Baseline for Fine-Grained Visual Classification. <span class="pub-badges"><a class="badge-tldr" href="./publications/mhem/index.html">TL;DR</a><a class="badge-paper" href="https://ieeexplore.ieee.org/abstract/document/9956020/" target="_blank" rel="noopener">Link</a></span> <br><b>Yuanzhi Liang</b>, Linchao Zhu, Xiaohan Wang, Yi Yang.<br> Accepted by TNNLS 2022 <br><br></li>

				<!-- <li>Towards Better Railway Service: Passengers Counting in Railway Compartment.  <br><b>Yuanzhi Liang</b>, Zhu Li, Xueming Qian.<br> Accepted by TCSVT 2020 <br><br></li> -->

				<li>Intersyn: Interleaved learning for dynamic motion synthesis in the wild <br>Yiyi Ma*, <b>Yuanzhi Liang</b>, Xiu Li, Chi Zhang, Xuelong Li.<br> Accepted by ICCV 2025 <br><br></li>
				
				<li>Freelong: Training-free long video generation with spectralblend temporal attention. <br>Yu Lu, <b>Yuanzhi Liang</b>, Linchao Zhu, Yi Yang.<br> Accepted by NeurIPS 2024 <br><br></li>

				<li>Removing Raindrops and Rain Streaks in One Go. <br>Ruijie Quan, Xin Yu, <b>Yuanzhi Liang</b>, Yi Yang.<br>Accepted by CVPR 2021 <br><br></li>

				<!-- <li>Product Recognition for Unmanned Vending Machines. <br>Chengxu Liu, Zongyang Da, <b>Yuanzhi Liang</b>, Yao Xue, Guoshuai Zhao, Xueming Qian.<br> Accepted by TNNLS 2022 <br><br></li> -->

				<li>Food and Ingredient Joint Learning for Fine-Grained Recognition.  <br>Chengxu Liu, <b>Yuanzhi Liang</b>, Yao Xue, Xueming Qian, Jianlong Fu.<br> Accepted by TCSVT 2020 <br><br></li>
				
				<p><i>Note *</i>: interns that I mentored.</p>

				
		</div>
      </div>


	  <a id="academic"></a>
	  <hr>

	  <div class="row">
		<div class="span12">
			<h3>Journal Reviewer</h3>
			<li>Reviewer for TPAMI and TIP.</li>
			<h3>Conference Reviewer / Program Committee Member</h3>
			<li>Reviewer for ICCV, CVPR, ICLR, NeurIPS, ECCV, MM, AAAI, IJCAI, ICME, and CAAI.</li>
			<h3>Others</h3>
			<li>Member of <a href="https://github.com/esbatmop/MNBVC">MNBVC (Massive Never-ending BT Vast Chinese corpus)</a>.</li>
		</div>
      </div>

	  <!--

	  <a id="cv"></a> <hr>
      <div class="row">
		<div class="span12">
			<h4>CV</h4>
			<p>
				More information about me: <a href="./cv/Yuanzhi Liang CV 2020.6.10.pdf">cv</a>	
			</p>
		</div>
      </div>

	  -->
      
	  <hr>

	  <div class="row">
		<div class="span12">
				<div align="center"> 
					<a href="https://clustrmaps.com/site/1bu57"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=k6KOcwz_2X8CFIy5DWZoKXDP4n2rVtUNCh82VD6Tw68&cl=ffffff" /></a>
				</div>
		</div>
      </div>

	</div> <!-- /container -->

    <!-- Le javascript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->

</body></html>
